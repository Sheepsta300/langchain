{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AzureContentSafetyTextTool`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.community.langchain_community.tools.azure_cognitive_services.content_safety import AzureContentSafetyTextTool\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_structured_chat_agent\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An agent needs a prompt to tell it what to do when given input, this pulls down a base prompt from LangSmith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGSMITH_KEY = os.environ['LANGSMITH_KEY']\n",
    "prompt = hub.pull(\"hwchase17/structured-chat-agent\", api_key=LANGSMITH_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `AzureContentSafetyTextTool` can be passed to an agent with a model and they will be used in conjunction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [AzureContentSafetyTextTool()]\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ['OPENAI_API_VERSION'],  \n",
    "    azure_deployment=os.environ['COMPLETIONS_MODEL'],\n",
    "    azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "    api_key=os.environ['AZURE_OPENAI_API_KEY'],\n",
    "    temperature=0.5,\n",
    "    max_tokens=200,\n",
    "    timeout=60,\n",
    "    max_retries=10,\n",
    "    )\n",
    "\n",
    "agent = create_structured_chat_agent(model, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tool inherits from LangChain cores `BaseTool` class which defines a class that can be used by a model if it decides it would be useful given a certain input. Below I mention wanting to check text for harmful content and the model decides the tool will be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"I hate you\"\n",
    "agent_executor.invoke({\"input\": f\"Can you check the following text for harmful content : {input}\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
