{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Services Toolkit\n",
    "\n",
    "This toolkit is used to interact with the `Azure AI Services API` to achieve some multimodal capabilities.\n",
    "\n",
    "Currently There are five tools bundled in this toolkit:\n",
    "- **AzureAiServicesImageAnalysisTool**: used to extract caption, objects, tags, and text from images.\n",
    "- **AzureAiServicesDocumentIntelligenceTool**: used to extract text, tables, and key-value pairs from documents.\n",
    "- **AzureAiServicesSpeechToTextTool**: used to transcribe speech to text.\n",
    "- **AzureAiServicesTextToSpeechTool**: used to synthesize text to speech.\n",
    "- **AzureAiServicesTextAnalyticsForHealthTool**: used to extract healthcare entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to set up an Azure account and create an AI Services resource. You can follow the instructions [here](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource) to create a resource. \n",
    "\n",
    "Then, you need to get the endpoint, key and region of your resource, and set them as environment variables. You can find them in the \"Keys and Endpoint\" page of your resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%pip install --upgrade --quiet  azure-ai-formrecognizer > /dev/null\n",
    "%pip install --upgrade --quiet  azure-cognitiveservices-speech > /dev/null\n",
    "%pip install --upgrade --quiet  azure-ai-textanalytics > /dev/null\n",
    "%pip install --upgrade --quiet  azure-ai-vision-imageanalysis > /dev/null\n",
    "%pip install -qU langchain-community"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-\"\n",
    "os.environ[\"AZURE_AI_SERVICES_KEY\"] = \"\"\n",
    "os.environ[\"AZURE_AI_SERVICES_ENDPOINT\"] = \"\"\n",
    "os.environ[\"AZURE_AI_SERVICES_REGION\"] = \"\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "from langchain_community.agent_toolkits import AzureAiServicesToolkit\n",
    "\n",
    "toolkit = AzureAiServicesToolkit()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "[tool.name for tool in toolkit.get_tools()]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use within an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_structured_chat_agent\n",
    "from langchain_openai import OpenAI"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "tools = toolkit.get_tools()\n",
    "prompt = hub.pull(\"hwchase17/structured-chat-agent\")\n",
    "agent = create_structured_chat_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What can I make with these ingredients? \"\n",
    "        + \"https://images.openai.com/blob/9ad5a2ab-041f-475f-ad6a-b51899c50182/ingredients.png\"\n",
    "    }\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "tts_result = agent_executor.invoke({\"input\": \"Tell me a joke and read it out for me.\"})\n",
    "audio_file = tts_result.get(\"output\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from IPython import display\n",
    "\n",
    "audio = display.Audio(data=audio_file, autoplay=True, rate=22050)\n",
    "display.display(audio)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "sample_input = \"\"\"\n",
    "The patient is a 54-year-old gentleman with a history of progressive angina over the past several months.\n",
    "The patient had a cardiac catheterization in July of this year revealing total occlusion of the RCA and 50% left main disease ,\n",
    "with a strong family history of coronary artery disease with a brother dying at the age of 52 from a myocardial infarction and\n",
    "another brother who is status post coronary artery bypass grafting. The patient had a stress echocardiogram done on July , 2001 ,\n",
    "which showed no wall motion abnormalities , but this was a difficult study due to body habitus. The patient went for six minutes with\n",
    "minimal ST depressions in the anterior lateral leads , thought due to fatigue and wrist pain , his anginal equivalent. Due to the patient's\n",
    "increased symptoms and family history and history left main disease with total occasional of his RCA was referred for revascularization with open heart surgery.\n",
    "\n",
    "List all the diagnoses.\n",
    "\"\"\"\n",
    "\n",
    "agent_executor.invoke({\"input\": sample_input})"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
